<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Resume - Start Bootstrap Theme</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
        type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-block d-lg-none">Mariya Sokolova</span>
            <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                    src="assets/img/profile.jpeg" alt="..." /></span>
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
                class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab1">Lab 1</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab2">Lab 2</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab3">Lab 3</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab4">Lab 4</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab5">Lab 5</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab6">Lab 6</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab7">Lab 7</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab8">Lab 8</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab9">Lab 9</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab10">Lab 10</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab11">Lab 11</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab12">Lab 12</a></li>
            </ul>
        </div>
    </nav>
    <!-- Page Content-->
    <div class="container-fluid p-0">
        <!-- About-->
        <section class="resume-section" id="about">
            <div class="resume-section-content">
                <h1 class="mb-0">
                    Mariya
                    <span class="text-primary">Sokolova</span>
                </h1>
                <div class="subheading mb-5">
                    Flora Rose House RM 231B· Ithaca, NY 14850 · (203) 252-9156 ·
                    <a href="mailto:name@email.com">ms2663@cornell.edu</a>
                </div>
                <p class="lead mb-5">My name is Mariya Sokolova and I am an Electrical and Computer Engineering student
                    at Cornell University graduating in December 2023. I have a wide variety of interests and I’m always
                    eager to explore anything that intrigues me, including robotics, micorelectronics, or lasers to name
                    a few. I'm also an artist, and I really enjoy using my creative skills to help me find unique
                    solutions to engineering problems!</p>
                <div class="social-icons">
                    <a class="social-icon" href="#!"><i class="fab fa-linkedin-in"></i></a>
                    <a class="social-icon" href="#!"><i class="fab fa-github"></i></a>
                    <a class="social-icon" href="#!"><i class="fab fa-facebook-f"></i></a>
                </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 1-->
        <section class="resume-section" id="lab1">
            <div class="resume-section-content">
                <h2 class="mb-5">Lab 1</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Artemis.</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The primary objective of Lab 1 is to serve as an introduction to the Artemis Nano board and
                            the Arduino IDE. The Artemis Nano is a development board created by Sparkfun Electronics to
                            use in small-scale projects. In ECE 4160, Artemis is programmed using Python and the Arduino
                            IDE. The lab involves installing the Arduino software and learning how to set up and use the
                            Artemis board by running a handful of example programs on it.</p>
                        <b>Materials:</b>
                        <ul>
                            <li>1 x SparkFun RedBoard Artemis Nano</li>
                            <li>1 x USB C-to-C or A-to-C cable</li>
                        </ul>
                        <b>Resources:</b>
                        <ul>
                            <li>SparkFun RedBoard Artemis Nano Product Page: https://www.sparkfun.com/products/15443
                            </li>
                            <li>Arduino IDE Download Page: https://wiki-content.arduino.cc/en/software</li>
                            <li>Artemis Setup Tutorial:
                                https://learn.sparkfun.com/tutorials/artemis-development-with-arduino?_ga=2.30055167.1151850962.1594648676-1889762036.1574524297&_gac=1.19903818.1593457111.Cj0KCQjwoub3BRC6ARIsABGhnyahkG7hU2v-0bSiAeprvZ7c9v0XEKYdVHIIi_-J-m5YLdDBMc2P_goaAtA4EALw_wcB
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <b>Installations.</b>
                        <p>The first step of the process was to complete the prelab, which involved installing the
                            Arduino IDE onto the computer being used for the lab. When you go to the Downloads page on
                            the Arduino website, there are several versions of the software available for different
                            operating systems. I used macOS with an Intel processor, so that is the version that I
                            installed.</p>
                        <img src="assets/img/lab_1/Downloads.jpeg" alt="Downloads" style="width:912px;height:400px;">
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>The Setup.</b>
                        <p>Once the software was downloaded, it was time to set everything up according to the
                            instructions referenced under “Resources” in the Introduction. First, I connected the
                            Artemis board to my computer using a USB-C. Next, I opened the IDE and installed the
                            SparkFun Apollo3 package. This package contains source files, libraries, and other tools
                            that would allow me to use the Arduino software with Artemis. Once that was done, I could
                            select the Artemis Nano board from the newly available board options.</p>
                        <img src="assets/img/lab_1/Selected.jpeg" alt="Selected" style="width:850px;height:400px;">
                        <b>Examples.</b>
                        <p>Now that everything was set up, it was time to gain familiarity with the Artemis board by
                            testing out a few example programs available on the IDE as well as some Artemis-specific
                            examples from the SparkFun package. The examples that were used for this lab were “Blink it
                            Up” from the IDE and “Example2_Serial”, “Example4-analogRead”, and
                            “Example1_MicrophoneOutput” from the package. For all the examples, I had to first manually
                            select the board, then press the “Verify” and “Upload” buttons on the top left corner of the
                            pop-up window.</p>
                        <img src="assets/img/lab_1/Choose.png" alt="Choose" style="width:615px;height:400px;">
                        <p>In the “Blink it Up” example, one of the LEDs on the board would blink every second. In the
                            code there is a loop, so the blinking doesn’t stop unless we stop the program. Below is an
                            image of the loop as well as a video of the blinking light.</p>
                        <img src="assets/img/lab_1/Exampleone.png" alt="Exampleone" style="width:615px;height:400px;">
                        <!--<video src="assets/img/Exvidone.png" alt="Exvidone" style="width:615px;height:400px;">-->
                        <p>In the “Example2_Serial” example, I was able to “talk” to the board by typing in messages in
                            the serial monitor and the Artemis would “answer” me by repeating what I wrote back to me.
                            Our “conversation” is shown below.</p>
                        <img src="assets/img/lab_1/Exampletwo.png" alt="Exampletwo" style="width:615px;height:400px;">
                        <p>In the “Example4-analogRead” example, the serial monitor continuously outputs the temperature
                            read (raw ADC) from the sensor. I could change the temperature by first rubbing (warming)
                            the board and then blowing (cooling) it. The temperature reads from these two actions are
                            shown below in the same order.</p>
                        <img src="assets/img/lab_1/Examplethreeone.png" alt="Examplethreeone"
                            style="width:615px;height:400px;">
                        <img src="assets/img/lab_1/Examplethreetwo.png" alt="Examplethreetwo"
                            style="width:615px;height:400px;">
                        <p>In the “Example1_MicrophoneOutput” the serial monitor continuously outputs the frequency
                            detected by the microphone. Below is a video showing how the frequency read changes as I hum
                            with various pitches.</p>
                        <!--<video src="assets/img/Exvidfour.png" alt="Exvidfour" style="width:615px;height:400px;">-->
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>The importance of Lab 1 was to allow students to gain some basic knowledge and skills for the
                            Artemis Nano board and the Arduino IDE. This experience provides a smooth transition into
                            more complex projects because it ensures that the board works properly for simple tasks.
                            Moreover, it demonstrated some capabilities of the Artemis board that could be applied to
                            future Labs in ECE 4160.</p>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                    </div>
                </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 2-->
        <section class="resume-section" id="lab2">
            <div class="resume-section-content">
                <h2 class="mb-5">Lab 2</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Bluetooth.</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The purpose of Lab 2 is to enable communication between the Artemis board and the computer
                            using Bluetooth, which was done so that in future labs Bluetooth can be used to send data.
                            The lab involved establishing the connection and then testing it by trying out several
                            different processing commands on the Artemis board.</p>
                        <b>Resources:</b>
                        <ul>
                            <li>Information About ArtemisBLE: https://www.arduino.cc/reference/en/libraries/arduinoble/
                            </li>
                            <li>https://zt88.github.io/Fast-Robots/lab2.html</li>
                            <li>https://klarinetkat.github.io/ECE4160/#lab2</li>
                            <li>https://rochelleb1.github.io/FastRobotsSP23/labpages/lab2.html</li>
                            <li>https://cwf54.github.io/#interests</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <b>The Setup.</b>
                        <p>The first step of the process was to complete the prelab, which consisted of several parts.
                            The first part involved installing Python 3, a virtual environment, and several python
                            packages. Next I downloaded the codebase, set it up, and organized the directory. After
                            this, I started the Jupyter server. Lastly, I installed ArduinoBLE using the library manager
                            in the Arduino IDE.</p>
                        <p>Before moving forward, it’s important to understand what Bluetooth is, what’s included in the
                            provided codebase, and how both of them work.</p>
                        <b>Bluetooth.</b>
                        <p>Bluetooth is a method of wireless communication where any device can obtain information from
                            available data. As described in the ArtemisBLE reference in the Introduction, there’s a
                            peripheral device that behaves similarly to a bulletin board: it contains all the data and
                            any device in the system can read it.</p>
                        <b>Codebase.</b>
                        <p>The codebase splits the lab into two main parts. First, we have the code in the Arduino IDE
                            (located in the ble_arduino folder), which is what gets run on the Artemis board. Second, we
                            have the Python code in the Jupyter Notebook (located in the ble_python folder). This is
                            where we run our commands including establishing the Bluetooth connection and running
                            example commands. Both folders have files that allow the two parts to work together. It’s
                            very important that both halves of the codebase are consistent with each other, otherwise
                            communication will fail.</p>
                        <p>At this point we could determine our board’s MAC address by running the ble_arduino.ino
                            sketch located in the ble_arduino folder in our codebase. The output of the program with the
                            board’s MAC address is shown here:</p>
                        <img src="assets/img/lab_2/mac_address.jpeg" alt="mac_address"
                            style="width:640px;height:400px;">
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>Configurations.</b>
                        <p>The first task was to change some configurations in the codebase. First, I copied the MAC
                            address obtained in the prelab and pasted it in the connection.yaml file in Jupyter. Next, I
                            changed out BLEService UUID so that my computer connected to the correct Artemis board. I
                            put this new UUID address in the connection.yaml file and the ble_arduino.ino sketch.</p>
                        <p>I obtained the new UUID by creating a new cell in my Jupyter notebook and running the
                            commands as illustrated below:</p>
                        <img src="assets/img/lab_2/generating_uuid.png" alt="generating_uuid"
                            style="width:640px;height:400px;">
                        <p>Next, I ensured that the addresses and the command types agree between conneciton.yaml and
                            ble_arduino. Below are images of the updated connection.yaml and ble_arduino.ino files, in
                            that order:</p>
                        <img src="assets/img/lab_2/updated_connection.yaml.png" alt="updated_connection.yaml"
                            style="width:985px;height:300px;">
                        <p></p>
                        <img src="assets/img/lab_2/updated_ble_arduino.ino.png" alt="updated_ble_arduino.ino"
                            style="width:515px;height:400px;">
                        <p>After changing the configurations, I ensured that the Bluetooth connection works properly by
                            running the demo.ipynb file in Jupyter. The two screenshots below demonstrate that I was
                            able to connect the two devices:</p>
                        <img src="assets/img/lab_2/connection_proof_jupyter.png" alt="connection_proof_jupyter"
                            style="width:920px;height:195px;">
                        <p></p>
                        <img src="assets/img/lab_2/connection_proof_arduino.png" alt="connection_proof_arduino"
                            style="width:520px;height:400px;">
                        <p>Below are two more images that prove that the demo.ipynb file ran successfully. First, we get
                            a screenshot of the "PING" and "SET_TWO_INTS" commands on Jupyter. Second, we have the
                            output of these commands in the arduino software. Both commands were executed successfully.
                        </p>
                        <img src="assets/img/lab_2/commands_try_j.png" alt="commands_try_j"
                            style="width:750px;height:320px;">
                        <p></p>
                        <img src="assets/img/lab_2/commands_try_a.png" alt="commands_try_a"
                            style="width:750px;height:210px;">
                        <p>After the Bluetooth connection was established, it was time to try a few commands.</p>
                        <div class="flex-grow-1"></div>
                        <b>Task 1.</b>
                        <p>The “Send Echo Command” takes as an input some message from the user and then prints it back
                            out, just like a real echo! To implement it in the ble_arduino.ino sketch, I copied and
                            pasted the code from the “Ping” command and changed two lines. Then, I wrote two commands in
                            the Jupyter file using the “Ping” command as an example:</p>
                        <img src="assets/img/lab_2/task_1.png" alt="task_1" style="width:869px;height:400px;">
                        <p>The next three commands are new, so for all of them I first had to add the name of each
                            command to CommandTypes in the ble_arduino.ino sketch and to the CMD class in cmd_types.py,
                            as shown:
                        </p>
                        <img src="assets/img/lab_2/command_types.png" alt="command_types"
                            style="width:205px;height:231px;">
                        <p></p>
                        <img src="assets/img/lab_2/cmd.png" alt="cmd" style="width:480px;height:400px;">
                        <div class="flex-grow-1"></div>
                        <b>Task 2.</b>
                        <p>The “Get Time Command” outputs the long “currentMillis” in the following format:
                            “T:(currentMillis)”. To implement it in the ble_arduino.ino sketch, I modified the code from
                            the “Ping'' command. Next, I wrote the new command in Jupyter:</p>
                        <img src="assets/img/lab_2/task_2.png" alt="task_2" style="width:903px;height:300px;">
                        <div class="flex-grow-1"></div>
                        <b>Task 3.</b>
                        <p>The “Notification Handler” allows updated data from the Artemis board to be continuously
                            printed out, which is very useful for the next two commands. To write the code, I took
                            inspiration from my classmates:</p>
                        <img src="assets/img/lab_2/task_3.png" alt="task_3" style="width:876px;height:300px;">
                        <div class="flex-grow-1"></div>
                        <b>Task 4.</b>
                        <p>The “GET_TEMP_5s” sends the internal die temperature readings for different time stamps in
                            the following format “T:(time)|C:(temperature)|...”. Measurements are taken once per second
                            for five seconds. To implement this, I created a loop that repeats 5 times and has a delay
                            of 1 second after every round to ensure that measurements are taken once per second:</p>
                        <img src="assets/img/lab_2/task_4.png" alt="task_4" style="width:615px;height:400px;">
                        <div class="flex-grow-1"></div>
                        <b>Task 5.</b>
                        <p>The “GET_TEMP_5s_RAPID” is almost like “GET_TEMP_5s”, except in this case we take as many
                            samples as possible over 5 seconds. Thus, to implement this I had to modify the
                            “GET_TEMP_5s” code to have a while loop that repeats until the current time is 5 seconds and
                            that doesn’t have a delay:</p>
                        <img src="assets/img/lab_2/task_5.png" alt="task_5" style="width:615px;height:400px;">
                        <p>Below are images of the Jupyter notebook and the output in the Serial Monitor, in that order,
                            once I implemented and ran all the examples described above.</p>
                        <img src="assets/img/lab_2/commands_j.png" alt="commands_j" style="width:521px;height:400px;">
                        <p></p>
                        <img src="assets/img/lab_2/commands_j2.png" alt="commands_j2" style="width:521px;height:400px;">
                        <p></p>
                        <img src="assets/img/lab_2/commands_a.png" alt="commands_a" style="width:604px;height:246px;">
                        <p></p>
                        <img src="assets/img/lab_2/commands_a2.png" alt="commands_a2" style="width:604px;height:246px;">
                        <p>It’s important to note that there is a limitation when it comes to Bluetooth communication.
                            Let’s take 5 seconds of 16-bit values taken at 150 Hz as an example. In total, we would
                            have:
                        </p>
                        <p>(5 seconds)*(150 times/second)*(16 bits)*(1 byte/8 bits) = 1500 bytes of data every 5
                            seconds.</p>
                        <p>We’re given that the Artemis board has 384000 bytes of RAM. This means that the maximum
                            number of 5-second cycles that we can have is the following:</p>
                        <p>(384000 bytes)/(1500 bytes) = 256</p>
                        <p>So the Artemis board can’t handle more than 256 rounds of such 5-second cycles.</p>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>The importance of Lab 2 was to enable a wireless method of communication between the computer
                            and the Artemis board. To do this, we used Bluetooth BLE, a wireless communication system
                            that allows data transfer between two devices. As a result, we now have a convenient way to
                            exchange information between robots to use in future labs.
                        </p>
                    </div>
                </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 3-->
        <section class="resume-section" id="lab3">
            <div class="resume-section-content">
                <h2 class="mb-5">Lab 3</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Time Of Flight Sensors.</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The goal of Lab 3 was to allow our robot to obtain information from the outside world. This
                            was done by attaching a varied (first one, then two) number of sensors to the Artemis board
                            and then testing them by obtaining data. Specifically, we attached Time-of-Flight (ToF) so
                            that our robot could detect how far it is from surrounding objects.</p>
                        <b>Materials:</b>
                        <ul>
                            <li>1 x SparkFun RedBoard Artemis Nano</li>
                            <li>1 x USB cable</li>
                            <li>2 x 4m ToF sensor</li>
                            <li>1 x QWIIC Breakout board</li>
                            <li>2 x Qwiic connector</li>
                            <li>1 x Ruler or graph paper</li>
                        </ul>
                        <b>Resources:</b>
                        <ul>
                            <li>https://zt88.github.io/Fast-Robots/index.html</li>
                            <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                            <li>https://cwf54.github.io/#interests</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <b>I2C Address.</b>
                        <p>The first step of the process was to complete the prelab, which involved reading through the
                            VL53L1X API user manual and data sheet while looking out for the I2C sensor address.</p>
                        <p>According to the datasheet, the I2C address is 0x52:</p>
                        <img src="assets/img/lab_3/i2c_address.png" alt="i2c_address" style="width:885px;height:380px;">
                        <div class="flex-grow-1"></div>
                        <b>Using Two Sensors.</b>
                        <p>In this lab, we eventually would use 2 ToF sensors in parallel. The advantage of using two
                            sensors rather than one is that with two sensors our robot would be able to obtain more
                            information and detect more obstacles than it would with just one sensor. For example, I
                            currently plan to put the sensors on the front left and the front right sides of the robot
                            so it could detect obstacles that are either in front, to the left, or to the right. That
                            way, it can react appropriately by either backing off or changing its trajectory.</p>
                        <b>Wiring Diagram.</b>
                        <p>Another task in the prelab was to plan out the wiring for some components in the lab. Below
                            is the wiring diagram, which includes the Artemis board, the Qwiic breakout board, and the
                            two ToF sensors:</p>
                        <img src="assets/img/lab_3/wiring_diagram.png" alt="wiring_diagram"
                            style="width:425px;height:318px;">
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>Installations.</b>
                        <p>First, I installed the parkFun VL53L1X 4m laser distance sensor library using the Arduino IDE
                            library manager:</p>
                        <img src="assets/img/lab_3/distance_sensor_lib.png" alt="distance_sensor_lib"
                            style="width:270px;height:515px;">
                        <p>This would allow me to test the ToF sensors using an example from this library later in the
                            lab.</p>
                        <b>Soldering.</b>
                        <p>Next, I soldered two ToF sensors to Qwiic connectors so that I could attach them to the
                            Artemis board via the Qwiic MultiPort breakout board. The Qwiic connectors each had four
                            wires: red, black, blue, and yellow. According to convention, I first soldered the red wire
                            to the voltage source and the black wire to the ground port. Next, I referred to the
                            schematic on the SparkFun Qwiic MultiPort web page to figure out how to solder the remaining
                            two wires. The order of the wires on the connectors gave away which port each one should be
                            connected to:</p>
                        <img src="assets/img/lab_3/wire_colors.png" alt="wire_colors" style="width:461px;height:307px;">
                        <p>And so I soldered the blue wire to SDA and the yellow wire to SCL. I ended up with two
                            sensors that looked like this:</p>
                        <img src="assets/img/lab_3/sensor_close.png" alt="sensor_close"
                            style="width:386px;height:408px;">
                        <p></p>
                        <img src="assets/img/lab_3/sensor_far.png" alt="sensor_far" style="width:764px;height:348px;">
                        <div class="flex-grow-1"></div>
                        <b>One Sensor.</b>
                        <p>The first experiment involved an Artemis board connected to just one sensor. To join them, I
                            attached both the sensor and the board to the Qwiic MultiPort breakout board using Qwiic
                            connectors. The resulting setup is shown below:</p>
                        <img src="assets/img/lab_3/one_sensor.png" alt="one_sensor" style="width:744px;height:520px;">
                        <p>Now that the physical components were ready, it was time to test them.</p>
                        <p>To start, I needed to ensure that we are using the correct I2C address and to familiarize
                            myself with I2C commands. So I connected the Artemis board and sensor setup to my computer
                            and ran the “Example1_wire_I2C” example from the Apollo3 library. As shown below, the
                            address does not match with what I expected; I expected 0x52, but instead got 0x29:</p>
                        <img src="assets/img/lab_3/i2c_ex.png" alt="i2c_ex" style="width:500px;height:360px;">
                        <p></p>
                        <img src="assets/img/lab_3/0x29.png" alt="0x29" style="width:804px;height:540px;">
                        <p>To do the experiment, I had to choose one of three modes for the maximum expected range of
                            the robot: short/default (1.3 m), medium (3 m) or long (4 m). The best mode to choose for an
                            experiment depends on how far and freely our robot is expected to move. For example, if we
                            allow our robot to freely roam around in a big room, it’s important to be aware of any
                            objects that are some distance away from the robot to avoid collisions. In this experiment,
                            I only intend to move the board a little bit in the vicinity of my computer. The
                            short/default mode is most appropriate for this case because we only care about objects that
                            are close to us. This is what I ended up going with for this lab:</p>
                        <img src="assets/img/lab_3/short_mode.png" alt="short_mode" style="width:670px;height:57px;">
                        <p>After setting everything up, it was time to test the (short) mode that I chose above using
                            the “Example1_ReadDistance” example from the VL53L1X 4m laser distance sensor library:</p>
                        <img src="assets/img/lab_3/distance_ex.png" alt="distance_ex" style="width:500px;height:360px;">
                        <p>For this part, I had to first ensure accuracy and repeatability of the sensor over some
                            range. To do that, I designed an experiment where I measured the distance of a large box at
                            various positions (10 mm, 50 mm, 100 mm, 150 mm, 200 mm, 250 mm, and 300 mm) and compared it
                            to the program output.</p>
                        <p>Here’s the measurement:</p>
                        <img src="assets/img/lab_3/measurement.png" alt="measurement" style="width:766px;height:535px;">
                        <p>Here’s the corresponding output:</p>
                        <img src="assets/img/lab_3/corr_out.png" alt="corr_out" style="width:692px;height:720px;">
                        <p>I did three trials of the experiment and plotted them below:</p>
                        <p></p>
                        <p>As we can see, the measurements are almost exact except for when we approach zero: the output
                            reads zero at around 10mm, before the sensor collides with the object.</p>
                        <p>I also had to find the ranging time with StopRanging, without StopRanging, and with just the
                            reading. To do so, I modified the code to compute the ranging time for those conditions:
                        </p>
                        <img src="assets/img/lab_3/time_one.png" alt="time_one" style="width:351px;height:232px;">
                        <p>Here is a table with my findings:</p>
                        <img src="assets/img/lab_3/time_table.png" alt="time_table" style="width:773px;height:202px;">
                        <div class="flex-grow-1"></div>
                        <b>Two Sensors.</b>
                        <p>After this, it was time to test two sensors at once. First, I had to first change the address
                            of one of the sensors because by default both have the same one: 0x29. To do this, I
                            connected the XSHUT pin of one sensor to one of the Artemis’s pins (green wire in the wiring
                            diagram), as shown below:</p>
                        <img src="assets/img/lab_3/two_sensors.png" alt="two_sensors" style="width:302px;height:403px;">
                        <p>Thereby, I could turn off that sensor to change the address of the other one. Below is the
                            code for that:</p>
                        <img src="assets/img/lab_3/change_add.png" alt="change_add" style="width:910px;height:562px;">
                        <p>I also had to modify the code so that the output showed readings from both sensors. The
                            output below demosntrates my successful implementation:</p>
                        <img src="assets/img/lab_3/two_sensor_out.png" alt="two_sensor_out"
                            style="width:508px;height:313px;">
                        <p>Next, I had to write a program that would allow me to quickly and continuously print the
                            Artemis clock to the Serial while only printing new ToF sensor data when it’s available. To
                            do this, I replaced the default while loop with two if-statements. That way, we only get a
                            reading for a sensor once it’s available. Here’s the code and its output:</p>
                        <img src="assets/img/lab_3/faster.png" alt="faster" style="width:864px;height:540px;">
                        <p></p>
                        <img src="assets/img/lab_3/faster_out.png" alt="faster_out" style="width:712px;height:518px;">
                        <p>As we can see from the output, our loop currently executes in 4 or 5 ms when there's no
                            reading and two times longer when there is a reading. The limiting factor is therefore just
                            checking whether there is a reading or not.</p>
                        <p>Last, I had to combine the code from this lab with the code from Lab 2 so that the ToF data
                            got sent over to my computer via Bluetooth. Consequently, the output would give time-stamped
                            distance readings instead of temperature readings. Below is my code:</p>
                          <img src="assets/img/lab_3/get_tof.png" alt="get_tof" style="width:711px;height:558px;">
                        <p>Here is the output of the code:</p>
                        <img src="assets/img/lab_3/bluetooth_0.png" alt="bluetooth_0" style="width:720px;height:397px;">
                        <img src="assets/img/lab_3/bluetooth_1.png" alt="bluetooth_1" style="width:720px;height:398px;">
                        <img src="assets/img/lab_3/bluetooth_2.png" alt="bluetooth_2" style="width:720px;height:398px;">
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>In Lab 3, we used a varied number of sensors to allow our robot to gather information
                            from its surroundings. This is a very important milestone because giving a robot some
                            way to perceive the environment is the first step for it to be able to interact with the
                            outside world. Moreover, this step gives us an opportunity to explore a variety of new
                            and interesting projects moving forward!
                        </p>
                    </div>
                </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 4-->
        <section class="resume-section" id="lab4">
            <div class="resume-section-content">
                <h2 class="mb-5">Lab 4</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">The IMU.</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The goal of Lab 4 was to add an internal measurement unit (IMU) to our robot. That way, we are able to obtain data about our robot’s motion and orientation with respect to the outside world.</p>
                        <b>Materials:</b>
                        <ul>
                            <li>1 x SparkFun RedBoard Artemis Nano</li>
                            <li>1 x USB cable</li>
                            <li>1 x 9DOF IMU sensor</li>
                            <li>2 x 4m ToF sensor</li>
                            <li>1 x QWIIC Breakout board</li>
                            <li>2 x Qwiic connector</li>
                            <li>1 x JST2 connector+cable</li>
                            <li>1 x Force1 RC car</li>
                            <li>1 x Li-Ion 3.7V 850mAh battery</li>
                        </ul>
                        <b>Resources:</b>
                        <ul>
                            <li>https://zt88.github.io/Fast-Robots/index.html</li>
                            <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                            <li>https://cwf54.github.io/#interests</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <p>The first step of the process was to complete the prelab, which involved reading through the ICM-20948 data sheet and just in general familiarizing oneself with the breakout board and the lab procedure.</p>
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>Set Up the IMU.</b>
                        <p>To begin, we had to set up the IMU. First, I installed the SparkFun 9DOF IMU Breakout - ICM 20948 - Arduino Library:</p>
                        <img src="assets/img/lab_4/imu_install.png" alt="imu_install"
                            style="width:280px;height:564px;">
                        <p>Next, I connected the IMU to my Artemis board using the QWIIC connectors like so:</p>
                        <img src="assets/img/lab_4/imu_connections.png" alt="imu_connections"
                            style="width:606px;height:702px;">
                        <p>Then, I ran the Example1_Basics code from that library to make sure that the IMU works:</p>
                        <img src="assets/img/lab_4/imu_ex_code.png" alt="imu_ex_code"
                            style="width:864px;height:540px;">
                        <p>Below is a screenshot of the serial monitor output, which shows data from the accelerometer and the gyroscope among other things. The accelerometer gives us x,y, and z values (mg) and the gyroscope gives us roll, pitch, and yaw (DPS).</p>
                        <img src="assets/img/lab_4/basics_works.png" alt="basics_works"
                            style="width:864px;height:540px;">
                        <p>I also made a screen recording of the serial plotter, which shows changes in values when the IMU is moved or rotated, which shows that the IMU responds to changes in motion and position.</p>
                        <p> </p>
                        <p><a href="https://youtu.be/Z-yov73dgz8">IMU Example Code Serial Plotter.</a></p>
                        <p> </p>
                        <!-- <iframe width="720" height="450"
                            src="https://youtu.be/Z-yov73dgz8">
                            </iframe> -->
                        <p>Note that in the code we see that the AD0_VAL definition is 1. This is expected because according to the data sheet this is the default value when the ADR jumper is not closed. If we wanted to change the I2C address, we would solder the ADR jumper, which would mean that the AD0_VAL definition would be 0. Since we are only using one IMU, we don’t need to do that.</p>
                        <p>Also, I slightly modified the code so that there were three LED blinks at the start, as shown in the video below. That way, we know that the code works!</p>
                        <p> </p>
                        <p><a href="https://youtu.be/ggAP2HXRiaU">LED Blinks.</a></p>
                        <p> </p>
                        <!-- <iframe width="640" height="360"
                            src="https://youtu.be/ggAP2HXRiaU">
                            </iframe> -->
                        <b>Accelerometer.</b>
                        <p>The next step is to test the accelerometer by measuring roll and pitch values for different orientations of the IMU. To do that, I first needed to convert the x, y, and z values into pitch and roll values, which I did by using the code posted on the course website.</p>
                        <p>First, I made measurements when the IMU is flat on the table:</p>
                        <img src="assets/img/lab_4/accel_0.png" alt="accel_0"
                            style="width:580px;height:700px;">
                        <img src="assets/img/lab_4/accel_0_out.png" alt="accel_0_out"
                            style="width:528px;height:495px;">
                        <p>Next, I made measurements when the IMU is flipped 90 degrees to the left:</p>
                        <img src="assets/img/lab_4/accel_1.png" alt="accel_1"
                            style="width:530px;height:570px;">
                        <img src="assets/img/lab_4/accel_1_out.png" alt="accel_1_out"
                            style="width:531px;height:495px;">
                        <p>Lastly, I made measurements when the IMU is flipped 90 degrees to the right:</p>
                        <img src="assets/img/lab_4/accel_2.png" alt="accel_2"
                            style="width:561px;height:627px;">
                        <img src="assets/img/lab_4/accel_2_out.png" alt="accel_2_out"
                            style="width:531px;height:504px;">
                        <p>As we can see above, the values are quite accurate, although there is slight noise. To analyze this noise, I collected some data and plotted it in the time domain and frequency domain as shown below, respectively.</p>
                        <img src="assets/img/lab_4/time_dom.png" alt="time_dom"
                            style="width:608px;height:462px;">
                        <img src="assets/img/lab_4/freq_dom.png" alt="freq_dom"
                            style="width:589px;height:456px;">
                        <p>As we can see, we don’t have a lot of noise because there is a built-in LPF in this particular model. This means that noise is reduced and we don’t need an additional low-pass filter.</p>
                        <b>Gyroscope.</b>
                        <p>The next phase was to assess the gyroscope by measuring pitch, roll, and yaw values for different orientations of the IMU. To do that, I first needed to compute the pitch, roll, and yaw values, which I did by using the code posted on the course website.</p>
                        <p>First, I made measurements when the IMU is flat on the table:</p>
                        <img src="assets/img/lab_4/gyr_0.jpg" alt="gyr_0"
                            style="width:508px;height:500px;">
                        <img src="assets/img/lab_4/gyr_0_out.png" alt="gyr_0_out"
                            style="width:613px;height:493px;">
                        <p>Next, I made measurements when the IMU is flipped 90 degrees to the left:</p>
                        <img src="assets/img/lab_4/gyr_1.jpg" alt="gyr_1"
                            style="width:570px;height:598px;">
                        <img src="assets/img/lab_4/gyr_1_out.png" alt="gyr_1_out"
                            style="width:611px;height:493px;">
                        <p>Lastly, I made measurements when the IMU is flipped 90 degrees to the right:</p>
                        <img src="assets/img/lab_4/gyr_2.jpg" alt="gyr_2"
                            style="width:550px;height:550px;">
                        <img src="assets/img/lab_4/gyr_2_out.png" alt="gyr_2_out"
                            style="width:611px;height:492px;">
                        <p>I also took a video recording of the serial plotter, from which we can see that the gyroscope measurements shift as time goes by:</p>
                        <p> </p>
                        <p><a href="https://www.youtube.com/watch?v=MFT6YPM-oI4">Gyroscope Serial Plotter.</a></p>
                        <p> </p>
                        <!-- <iframe width="720" height="450"
                            src="https://www.youtube.com/watch?v=MFT6YPM-oI4">
                            </iframe> -->
                        <p>We can also see this in the measurements; for example when the IMU is flat, the pitch measurement is around -17 when it should be close to zero, meaning that the gyroscope isn’t very accurate. Even though we have a built-in LPF, there is still some noise which can be reduced by implementing a complementary filter:</p>
                        <img src="assets/img/lab_4/gyr_cf.png" alt="gyr_cf"
                            style="width:608px;height:376px;">
                        <p>After this, I redid the measurements for all three positions:</p>
                        <p>When the IMU is flat on the table:</p>
                        <img src="assets/img/lab_4/gyr_0.jpg" alt="gyr_0"
                            style="width:508px;height:500px;">
                        <img src="assets/img/lab_4/gyr_0_redo.png" alt="gyr_0_redo"
                            style="width:611px;height:465px;">
                        <p>When the IMU is flipped 90 degrees to the left:</p>
                        <img src="assets/img/lab_4/gyr_1.jpg" alt="gyr_1"
                            style="width:570px;height:598px;">
                        <img src="assets/img/lab_4/gyr_1_redo.png" alt="gyr_1_redo"
                            style="width:611px;height:465px;">
                        <p>When the IMU is flipped 90 degrees to the right:</p>
                        <img src="assets/img/lab_4/gyr_2.jpg" alt="gyr_2"
                            style="width:550px;height:550px;">
                        <img src="assets/img/lab_4/gyr_2_redo.png" alt="gyr_2_redo"
                            style="width:611px;height:465px;">
                        <p>As we can see, our measurements are much better and more accurate.</p>
                        <b>Sample Data.</b>
                        <p>For this section, I first had to figure out how fast I am able to sample new values. To do that, I printed out the time stamps before the data was collected and after the data was collected:</p>
                        <img src="assets/img/lab_4/timing.png" alt="timing"
                            style="width:829px;height:518px;">
                        <p>As we can see, it takes about 2-3 milliseconds. Note that this was after I removed all other print statements to speed up the data collection process.</p>
                        <p>Next, I used bluetooth to collect time-stamped IMU data via Bluetooth and save it into an array:</p>
                        <p>Below is my Arduino code, python code, and the output that it generated in Jupyter:</p>
                        <img src="assets/img/lab_4/get_imu.png" alt="get_imu"
                            style="width:782px;height:679px;">
                        <img src="assets/img/lab_4/gi.png" alt="gi"
                            style="width:864px;height:478px;">
                        <p>Then, I used bluetooth to collect 5s worth of time-stamped IMU data along with ToF data, which I stored in separate arrays. I did this because having the data separated is more convenient for making plots; instead of having to index, I can just refer to whatever I want to plot.</p>
                        <p>Below is my Arduino code, python code, and the output that it generated in Jupyter:</p>
                        <img src="assets/img/lab_4/get_tof_imu.png" alt="get_tof_imu"
                            style="width:767px;height:1064px;">
                        <img src="assets/img/lab_4/gti_0.png" alt="gti_0"
                            style="width:720px;height:398px;">
                        <img src="assets/img/lab_4/gti_1.png" alt="gti_1"
                            style="width:720px;height:398px;">
                        <img src="assets/img/lab_4/gti_2.png" alt="gti_2"
                            style="width:720px;height:398px;">
                        <p> </p>
                        <b>Cut the Cord!</b>
                        <p>In this lab we are given two kinds of batteries. The first one is 3.7V 850mAh, which we use to power motors via the motor driver. The second one is 3.7V 650mAh, which we use to power the Artemis and other digital electronics. To make the 650mAh battery compatible with the Artemis board, I had to solder the cables from the battery to JST connectors as shown below:</p>
                        <img src="assets/img/lab_4/cut_the_cord.jpg" alt="cut_the_cord"
                            style="width:542px;height:416px;">
                        <p> </p>
                        <b>Record a Stunt.</b>
                        <p>The last part of the lab was to play with the toy car! First, I attached the 850mAh battery to the toy car and spent a few minutes testing it out on its own, as shown in the video below:</p>
                        <p>(Note: I disassembled my car for Lab 5 before realizing I have to do this, so I sadly don't have footage for this. However, I am quite confident that I can play with toy cars!)</p>
                        <p>Next, I attached the 650mAh battery to the Artemis board, after which I attached the board and the other components to the car. I spent some time movinging the car around like this, as shown in the video below:</p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/nrLVVdnsYjc?feature=share">The Stunt!</a></p>
                        <p> </p>
                        <!-- <iframe width="720" height="450"
                            src="https://youtube.com/shorts/nrLVVdnsYjc?feature=share">
                            </iframe> -->
                        <p>Also, while playing with the car, I collected some bluetooth data from the IMU, which is shown in the graphs below:</p>
                        <img src="assets/img/lab_4/get_tof_imu.png" alt="get_tof_imu"
                            style="width:818px;height:1134px;">
                        <img src="assets/img/lab_4/d1_plt.png" alt="d1_plt"
                            style="width:613px;height:467px;">
                        <img src="assets/img/lab_4/d2_plt.png" alt="d2_plt"
                            style="width:633px;height:463px;">
                        <img src="assets/img/lab_4/ap_plt.png" alt="ap_plt"
                            style="width:695px;height:469px;">
                        <img src="assets/img/lab_4/ar_plt.png" alt="ar_plt"
                            style="width:702px;height:469px;">
                        <img src="assets/img/lab_4/gp_plt.png" alt="gp_plt"
                            style="width:726px;height:469px;">
                        <img src="assets/img/lab_4/gr_plt.png" alt="gr_plt"
                            style="width:690px;height:471px;">
                        <img src="assets/img/lab_4/gy_plt.png" alt="gy_plt"
                            style="width:750px;height:463px;">
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>In Lab 4, we enabled our robot to measure its own orientation in space and motion with respect to the outside world. This was a logical next step from the previous lab, where we allowed our robot to gather information from its surroundings. Having information about its environment in combination with the ability to determine one’s own orientation in its environment is a major milestone for our robot; it gains many new capabilities and gives us a lot more possibilities to explore.
                        </p>
                    </div>
                </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 5-->
        <section class="resume-section" id="lab5">
            <div class="resume-section-content">
                <h2 class="mb-5">Lab 5</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Motors and Open Loop Control.</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>Coming Soon!</p>
                        <b>Materials:</b>
                        <ul>
                            <li>1 x SparkFun RedBoard Artemis Nano
                            <li>1 x USB cable
                            <li>1 x 9DOF IMU sensor</li>
                            <li>2 x 4m ToF sensor</li>
                            <li>1 x QWIIC Breakout board</li>
                            <li>2 x Qwiic connector</li>
                            <li>1 x JST2 connector+cable</li>
                            <li>1 x Force1 RC car</li>
                            <li>1 x Li-Ion 3.7V 850mAh battery</li>
                            <li>2 x Dual motor drivers</li>
                        </ul>
                        <b>Resources:</b>
                        <ul>
                            <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                            <li>https://cwf54.github.io/#interests</li>
                            <li>https://pages.github.coecis.cornell.edu/rc627/ECE-4960/</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <p>The primary objective of the prelab is to design a way to wire together the different hardware components. The goal is to parallel-couple the inputs and outputs of the motor drivers so that we can maximize power and the speed of our robot. To do this, we had to overview the documentation and datasheet for the dual motor driver. I also referred to Ryan Chan’s website from last year. Here is the wiring diagram that I ended up with:</p>
                        <img src="assets/img/lab_5/wiring_diagram.jpeg" alt="wiring_diagram"
                            style="width:626px;height:694px;">
                        <p>Like Ryan Chan, I used pins 4 and A5 for one motor driver and pins A14 and A15 for the other one because all these pins can generate PWM signals.</p>
                        <p>The Artemis and motor drivers should be powered from different batteries. This is because we want to maximize the amount of power given to the drivers and minimize power supply noise to the Artemis.</p>
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>Oscilloscope Test.</b>
                        <p>First, I used the wiring diagram that I came up with in the prelab to create connections between the Artemis board and the motor drivers, as shown below:</p>
                        <img src="assets/img/lab_5/wiring_pic.jpg" alt="wiring_pic"
                            style="width:605px;height:806px;">
                        <p>The next step was to show that we can regulate the power on the motor driver output. To do that, we had to test our motor drivers using an oscilloscope. First, I had to write a program using analogWrite to generate PWM signals:</p>
                        <img src="assets/img/lab_5/osc_code.png" alt="osc_code"
                            style="width:559px;height:351px;">
                        <p>Then, I connected one of the motor drivers to a power supply and an oscilloscope, as shown below:</p>
                        <img src="assets/img/lab_5/pwr_setup.jpg" alt="pwr_setup"
                            style="width:604px;height:400px;">
                        <p>I set the power supply to be 3.7 V to match the nominal voltage of the battery that we are using for our robot. I tested out both motor drivers by changing the pin numbers in the code above. Here are the results:</p>
                        <img src="assets/img/lab_5/pwr_0.jpg" alt="pwr_0"
                            style="width:794px;height:525px;">
                        <img src="assets/img/lab_5/osc_0.jpg" alt="osc_0"
                            style="width:743px;height:504px;">
                        <img src="assets/img/lab_5/pwr_1.jpg" alt="pwr_1"
                            style="width:750px;height:533px;">
                        <img src="assets/img/lab_5/osc_1.jpg" alt="osc_1"
                            style="width:781px;height:586px;">
                        <p>As we can see, we have square waves for both motor drivers, both work properly!</p>
                        <b>Assembling the Car!</b>
                        <p>Once we made sure that our motors can generate PWM signals, it was time to make sure that the wheels can spin forward and backwards. To do that, I first wrote code that would enable one set of wheels spin forward and backwards:</p>
                        <img src="assets/img/lab_5/two_wheels_code.png" alt="two_wheels_code"
                            style="width:475px;height:800px;">
                        <p>Next, I took a video of two wheels spinning in both directions:</p>
                        <p> </p>
                        <p><a href="https://youtu.be/RkiTnHLLx5A">Two Wheels Spinning.</a></p>
                        <p> </p>
                        <p>Once I knew that I could make one pair of wheels spin properly, I tried to do the same for all four wheels while also using the 850mAh battery to power the motor drivers. Below are the code that I used and a video of the wheels spinning, respectively.</p>
                        <img src="assets/img/lab_5/four_wheels_code.png" alt="four_wheels_code"
                            style="width:475px;height:800px;">
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/R3ZxyOaoIcY?feature=share">Four Wheels Spinning.</a></p>
                        <p> </p>
                        <p>Next, it was time to assemble the car! I inserted all the components, including the Artemis, the ToF sensors, the IMU and the motor drivers into the car, like so:</p>
                        <img src="assets/img/lab_5/car_wiring_0.jpg" alt="car_wiring_0"
                            style="width:500px;height:540px;">
                        <img src="assets/img/lab_5/car_wiring_1.jpg" alt="car_wiring_1"
                            style="width:500px;height:667px;">
                        <b>Controlling the Car.</b>
                        <p>The last part of the lab was to test our robot! The first step was to find the lower limit of the PWM duty cycle, or in other words, what’s the smallest signal I can send to my robot and still have the wheels rolling on the ground. I found this value to be about 45, which is about 17.65% of the duty cycle. Below is a video of the robot moving with this PWM value:</p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/Yot3emd_Vrs">Low PWM Limit.</a></p>
                        <p> </p>
                        <p>Next, I tested my robot to see if it can travel in a straight line, meaning that the right and left wheels spin at the same rate. I found there to be a slight difference in the wheel speeds: the right wheels turn slightly faster than the left ones, which makes the car move slightly to the left. I corrected this by multiplying the PWM of the right motor by a calibration factor of 1.3 to get the PWM of the left motor. Here is a video of the robot traveling in a straight line after all necessary adjustments (note  that the distance between the white tape and the brown tape is somewhere between 6 and 7 feet):</p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/30GdvCbddR0?feature=share">Robot Going Straight.</a></p>
                        <p> </p>
                        <p>Lastly, I had to demonstrate open-loop, untethered control of my robot. To do this, I first had to program the robot to complete a sequence of tasks. Below is my code, with the comments explaining what I wanted the robot to do in each step:</p>
                        <img src="assets/img/lab_5/open_loop_0.png" alt="open_loop_0"
                            style="width:500px;height:755px;">
                        <p> </p>
                        <img src="assets/img/lab_5/open_loop_1.png" alt="open_loop_1"
                            style="width:500px;height:500px;">
                        <p> </p>
                        <img src="assets/img/lab_5/open_loop_2.png" alt="open_loop_2"
                            style="width:500px;height:150px;">
                        <p> </p>
                        <p>Below is the video of my robot doing these tasks:</p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/7qU57KJb2uk">Open-Loop, Untethered Control.</a></p>
                        <p> </p>
                        <p>As we can see, the robot’s movements were consistent with the code!</p>
                        <p> </p>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>In Lab 5, we replaced manual control of the toy car with open loop control by replacing the original board in the toy car with the hardware that we created over the past few labs. This is a major milestone in our robot development journey because it combines everything that we have done in ECE 4160 so far and paves way for new and interesting experiments!
                        </p>
                    </div>
                 </div>
              </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 6-->
        <section class="resume-section" id="lab6">
           <div class="resume-section-content">
              <h2 class="mb-5">Lab 6</h2>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <h3 class="mb-0">Closed-Loop Control (PID).</h3>
                      <div class="subheading mb-3">Introduction.</div>
                      <p>The goal of Lab 6 was to integrate PID closed-loop control, which helps keep the robot motion consistent when the environment or dynamics change. For this lab, we had to choose between two tasks: position control and orientation control; I chose to do position control.</p>
                      <b>Materials:</b>
                      <ul>
                          <li>1 x R/C stunt car</li>
                          <li>1 x SparkFun RedBoard Artemis Nano</li>
                          <li>1 x USB cable</li>
                          <li>2 x Li-Po 3.7V 650mAh (or more) battery</li>
                          <li>2 x Dual motor drivers</li>
                          <li>2 x 4m ToF sensor</li>
                          <li>1 x 9DOF IMU sensor</li>
                          <li>2 x Qwiic connector</li>
                      </ul>
                      <b>Resources:</b>
                      <ul>
                          <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                          <li>https://cwf54.github.io/#interests</li>
                          <li>https://anyafp.github.io/ece4960/labs/lab6/</li>
                      </ul>
                  </div>
              </div>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <div class="subheading mb-3">Procedure.</div>
                      <b>THE PRELAB.</b>
                      <p>The primary objective of the prelab is to design a way to facilitate debugging. And so to prepare for this lab, I made a  copy of my ECE 4160 folder, created a new virtual environment, and changed the structure of my code. In previous labs, I had most of my code in cases that I would call in Jupyter and to obtain data, it would be sent to my computer in real time. This was not a very good structure because sending bluetooth data in real time is time-consuming and inefficient.</p>
                      <p>To solve the problems above, I decided to change the structure of my code. In my new code, I am essentially creating a separation of tasks; I collect the data in the main loop using the Artemis while the robot is performing tasks and I have a separate case to send the data over to my computer once the stunts are complete. I also have a few short cases to change some global variables as needed.</p>
                      <p>Here is the code that I wrote for the prelab:</p>
                      <img src="assets/img/lab_6/pre_0.png" alt="pre_0"
                          style="width:569px;height:483px;">
                      <p> </p>
                      <p>The first image shows the cases that I wrote for this lab. The first one, LAB_6, sends data from the Artemis to the computer in a string. The other two, START_RECORD and STOP_RECORD change the value of a global variable called start_record. When start_record is true, the robot is performing stunts and collecting data. When it’s set to false, it stops everything.</p>
                      <img src="assets/img/lab_6/pre_1.png" alt="pre_1"
                          style="width:548px;height:721px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>The second image shows my main loop at the beginning of the lab, before I added the P control code. That is where I would collect the data as well as control the robot’s movement. Here, instead of sending the data to the computer in real time, I store it in arrays so that I could send it over later.</p>
                      <p> </p>
                      <b>THE LAB TASKS.</b>
                      <p>To complete this task, I chose to focus on position control. As a 4000-level student, I was able to choose between P, PI, PID, and PD control, from which I chose to do P control. I didn’t go further than this because in my case, P control was enough to ensure that my robot completed the required task (stopping in response to a wall).</p>
                      <p>The reason why a lot of people do more than just P control is because you can’t guarantee that there will be zero steady state error. So in my case, there isn't 100% certainty that the robot will be exactly 1 ft away from the wall. However, I did several trials and I had little steady state error as my robot was consistently stopped around 1 ft away from the wall. And so in my case I felt that just P control was enough.</p>
                      <p>Below is my program, which uses the distance (in mm) from the front TOF sensor to continuously compute new PWM values:</p>
                      <img src="assets/img/lab_6/p_control.png" alt="p_control"
                          style="width:482px;height:614px;">
                      <p> </p>
                      <p>In my code, I start my computing the error by finding the difference between the current ToF reading and 304.5 mm, which is approximately equal to 1 ft. Next, I use this value to find the new PWM value. Depending on whether the car is farther or closer than 1 ft from the wall, I program the car to either keep moving toward the wall or track back. I also take into account the lower PWM limit (to account for friction).</p>
                      <p>To choose a Kp value, I did a lot of rigorous testing until I found a value that consistently ensured that my robot stopped one foot away from the wall. In other words, I wanted to select a value of Kp that would minimize the steady state error. To do that, I created a case that would allow me to change the Kp value by calling a case through jupyter, which allowed me to test many different Kp values without having to recompile my code. I tried several values between 1 and nearly zero. The values that did not work either caused my robot to be unstable or resulted in a high steady state error. I eventually settled on a Kp value of 0.1 because it worked well.</p>
                      <p>My Jupyter code included several parts. The first part is where I am able to change the Kp value. The second part is where I control when to start and stop recording data. Finally, I implemented a notification handler which obtains data from the Artemis once I stop recording data. Below is my code:</p>
                      <img src="assets/img/lab_6/jupyter.png" alt="jupyter"
                          style="width:960px;height:600px;">
                      <p> </p>
                      <p>Finally, once I selected an appropriate value for Kp, I performed three runs to ensure that my program was reliable. For each trial, I recorded a video of the robot and I made two plots, one for the ToF reading over time and one for the motor input value over time. The videos with the corresponding plots are shown below:</p>
                      <p>Trial 1:</p>
                      <p> </p>
                      <p><a href="https://youtu.be/RvVkUZQgf-Y">Trial 1.</a></p>
                      <p> </p>
                      <img src="assets/img/lab_6/t_1_0.png" alt="t_1_0"
                          style="width:603px;height:464px;">
                      <p> </p>
                      <img src="assets/img/lab_6/t_1_1.png" alt="t_1_1"
                          style="width:608px;height:466px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>Trial 2:</p>
                      <p> </p>
                      <p><a href="https://youtu.be/67uL1quL_2Q">Trial 2.</a></p>
                      <p> </p>
                      <img src="assets/img/lab_6/t_2_0.png" alt="t_2_0"
                          style="width:612px;height:464px;">
                      <p> </p>
                      <img src="assets/img/lab_6/t_2_1.png" alt="t_2_1"
                          style="width:616px;height:466px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>Trial 3:</p>
                      <p> </p>
                      <p><a href="https://youtu.be/OogZh5jeHaQ">Trial 3.</a></p>
                      <p> </p>
                      <img src="assets/img/lab_6/t_3_0.png" alt="t_3_0"
                          style="width:618px;height:466px;">
                      <p> </p>
                      <img src="assets/img/lab_6/t_3_1.png" alt="t_3_1"
                          style="width:608px;height:465px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>As we can see from all the Distance vs Time graphs above, the robot was consistently going towards around 300 mm away from the wall, which is almost equivalent to 1 ft. From the PWM vs Time graphs, we can see that as we get closer to the target distance, the PWM value approaches around 50. This is slightly higher than 45, which is what I found to be the lower PWM limit in Lab 5. The data above demonstrates that my code is sufficient for the robot to complete the task.</p>
                      <p> </p>
                  </div>
              </div>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <div class="subheading mb-3">Conclusion.</div>
                      <p>The objective of Lab 6 was to gain experience with PID or closed-loop control. To do this, I chose to complete Task A, which focuses on position control. In the end, I enabled my robot to stop in response to a wall in its trajectory. This is a significant milestone because it’s the first time that my robot was able to interact with the outside environment.
                      </p>
                    </div>
                 </div>
              </div>
        </section>
        <hr class="m-0" />
        <!-- Lab 7-->
        <section class="resume-section" id="lab7">
          <div class="resume-section-content">
            <h2 class="mb-5">Lab 7</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Kalman Filter.</h3>
                    <div class="subheading mb-3">Introduction.</div>
                    <p>The objective of Lab 7 was to execute our chosen task from Lab 6 more quickly by implementing a Kalman Filter. Because in Lab 6 I chose Task A (position control), my goal was to enable my robot to speed towards the wall but stop abruptly around 1 foot before hitting the wall.</p>
                    <b>Materials:</b>
                    <ul>
                      <li>1 x R/C stunt car</li>
                      <li>1 x SparkFun RedBoard Artemis Nano</li>
                      <li>1 x USB cable</li>
                      <li>2 x Li-Po 3.7V 650mAh (or more) battery</li>
                      <li>2 x Dual motor drivers</li>
                      <li>2 x 4m ToF sensor</li>
                      <li>1 x 9DOF IMU sensor</li>
                      <li>2 x Qwiic connector</li>
                    </ul>
                    <b>Resources:</b>
                    <ul>
                        <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                        <li>https://cwf54.github.io/#interests</li>
                        <li>https://anyafp.github.io/ece4960/labs/lab7/</li>
                    </ul>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <div class="subheading mb-3">Procedure.</div>
                    <b>THE PRELAB.</b>
                    <p>For this lab we did not have a pre-lab.</p>
                    <div class="flex-grow-1"></div>
                    <b>THE LAB TASKS.</b>
                    <b>Estimating Drag and Momentum.</b>
                    <p>The first part of the lab was to estimate drag and momentum for the A and B matrices using the equations below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/equations.png" alt="equations"
                        style="width:340px;height:240px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>We could find these values by using a step response. This involves driving the robot towards the wall while taking TOF data and PWM data until it reaches steady state. To do that, I programmed the robot to drive towards a wall for 1.2 seconds and stop before it hits the wall to avoid damaging the toy car. The reason why I chose 1.2 seconds is because after several trials I determined that this is the time that it took my robot to come close to steady state. Below is my Arduino code:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/d&m_arduino.png" alt="d&m_arduino"
                        style="width:730px;height:563px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>And here is a video of my robot while I was taking the measurement:</p>
                    <p> </p>
                    <p><a href="https://youtube.com/shorts/1Oj7quaQyc4?feature=share">Taking Measurement for Matrices.</a></p>
                    <p> </p>
                    <p>After I took my measurements, I processed the data in Jupyter to obtain a Distance vs Time plot, a Velocity vs Time plot, and a PWM vs Time plot. The three plots along with the code that I used to create them are shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/dist_plt.png" alt="dist_plt"
                        style="width:585px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_7/dist_code.png" alt="dist_code"
                        style="width:595px;height:336px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_7/vel_plt.png" alt="vel_plt"
                        style="width:600px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_7/vel_code.png" alt="vel_code"
                        style="width:595px;height:385px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_7/pwm_plt.png" alt="pwm_plt"
                        style="width:585px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_7/pwm_code.png" alt="pwm_code"
                        style="width:385px;height:123px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Note that practically we aren’t actually able to achieve steady state because there’s a limit to how far we could place the robot away from the wall while still obtaining TOF data, which is why the Velocity vs. Time plot has a sharp trough.</p>
                    <p>Once I had my plots, I could use my Velocity vs Time plot to approximate my drag and momentum values. To do that, I annotated the Velocity vs Time plot as shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/vel_plt_ann.png" alt="vel_plt_ann"
                        style="width:600px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>From this plot, we can approximate the steady state velocity to be 1800 mm/s. The 90% rise time is 90% of the steady state velocity, which is the velocity that the car reaches once it fully accelerates. To find 90% rise time, I calculated 90% of 1800 mm/s, which turned out to be 1620 mm/s, and approximated how long it takes to get to that velocity. As seen in the above plot, I found my 90% rise time to be around 1s.</p>
                    <p>Finally, I had all the necessary values to calculate the values for drag and momentum, which I can use to find my A and B matrices. Below are my calculations:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/calculations.png" alt="calculations"
                        style="width:700px;height:200px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>My final A and B matrices are shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/a_b_matrices.png" alt="a_b_matrices"
                        style="width:446px;height:284px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>For the C matrix, I used the one given in lecture:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/c_matrix.png" alt="c_matrix"
                        style="width:225px;height:75px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Lastly, I had to determine the process noise and sensor noise covariance matrices illustrated below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/noise_matrices.png" alt="noise_matrices"
                        style="width:500px;height:193px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>To do that, I had to choose three sigma values. I started off with the following values as an estimate:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/sigma_vals.png" alt="sigma_vals"
                        style="width:375px;height:75px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <b>The Kalman Filter.</b>
                    <p>Once I determined the A and B matrices, it was time for me to implement the Kalman filter. Below is the Jupyter code with all the necessary matrices and a modified version of the Kalman Filter function given in the lab instructions:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/kf_0.png" alt="kf_0"
                        style="width:600px;height:650px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Below is the code where I use the Kalman Filter function on my data:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/kf_1.png" alt="kf_1"
                        style="width:500px;height:213px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>For this part of the lab we needed to perform a sanity check on data taken during Lab 6 to make sure that the Kalman Filter is working properly. Because during Lab 6 I used a broken TOF sensor, I had to take new data and perform the sanity check on that. I just re-used my P control code from lab 6 to take new data. Below is a screenshot of my code and a video recording of the robot:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/sanity_check.png" alt="sanity_check"
                        style="width:300px;height:700px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p><a href="https://youtube.com/shorts/wKpg0uQkqWk?feature=share">Taking Measurement for Sanity Check.</a></p>
                    <p> </p>
                    <p>Once I had my data, I was able to plot the (measured) Distance vs Time plot, the (measured) Velocity vs Time plot, the PWM vs Time plot, and the Kalman Filter results. Below are all the plots in the listed order:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/kf_2.png" alt="kf_2"
                        style="width:475px;height:625px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <b>Extrapolation.</b>
                    <p>The last part of this lab was to choose between implementing the Kalman Filter on the robot and writing an extrapolation algorithm. Because I was short on time, I decided to create an extrapolation algorithm that would be useful for Lab 8.</p>
                    <p>To create this algorithm, I needed to subdivide my code into two parts.  The first part of my code deals with the case when the TOF sensor data is available. In that section, I continuously update two variables as I obtain new data: distance_prev, the distance measurement from the previous iteration, and distance_curr, the new measured distance. These values would be used for the extrapolation calculation. Here is the code for this part:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/pt_1.png" alt="pt_1"
                        style="width:700px;height:750px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>The second part of my code deals with the case when data is not available. There, I use distance_prev and distance_curr to calculate the estimated distance value, distance_est. I also update distance_prev and distance_curr by setting distance_prev to distance_curr and distance_curr to distance_est so that I can keep extrapolating until the TOF data is available again.  Here is the code for this part:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/pt_2.png" alt="pt_2"
                        style="width:525px;height:705px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>I also had to create another case that would send the extrapolated distance values because there would be a lot more of them than the measured data values. The Arduino code for that is shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/send_ext.png" alt="send_ext"
                        style="width:560px;height:215px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>To test my extrapolation code, I used the same data that I did for the Sanity Check part when implementing the Kalman Filter. Below I have a Distance vs Time plot that shows the overlapping measured distance and estimated distance values as well as the code I used to obtain the graph:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/both_dis_plt.png" alt="both_dis_plt"
                        style="width:475px;height:650px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>I also plotted the measured distance and the estimated distance of two separate plots. Below are screenshots of the Measured Distance vs Time plot and Extrapolated Distance vs Time plot, in that order:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/dis_meas_plt.png" alt="dis_meas_plt"
                        style="width:475px;height:625px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <img src="assets/img/lab_7/dis_est_plt.png" alt="dis_est_plt"
                        style="width:475px;height:625px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Lastly, I plotted the Velocity vs Time plot and the PWM vs Time plot for the measured data. Both are illustrated below, in that order:</p>
                    <p> </p>
                    <img src="assets/img/lab_7/meas_vel_plt.png" alt="meas_vel_plt"
                        style="width:475px;height:600px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <img src="assets/img/lab_7/meas_pwm_plt.png" alt="meas_pwm_plt"
                        style="width:450px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <div class="subheading mb-3">Conclusion.</div>
                    <p>In Lab 7, I had to implement a Kalman Filter to execute my chosen task from Lab 6 more quickly. In the end, my robot was able to speed towards the wall but stop abruptly around 1 foot before hitting it. This is very important because we want our robots to respond quickly to obstacles and therefore avoid collisions. This will prove useful in Lab 8, where we have to record a stunt that proves that this feature of our robot works well.
                    </p>
                  </div>
               </div>
            </div>
      </section>
      <hr class="m-0" />
        <!-- Lab 8 -->
        <section class="resume-section" id="lab8">
           <div class="resume-section-content">
              <h2 class="mb-5">Lab 8</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Stunts!</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The purpose of Lab 8 was to showcase what I have achieved in this class so far by programming the robot car to perform a stunt! I chose to perform Task A, which means that my stunt involves having the car rush towards the wall as fast as possible, perform a flip on a sticky mat placed ~0.5 m away from the wall, and drive back to its original position as fast as possible.</p>
                        <b>Materials:</b>
                        <ul>
                          <li>1 x R/C stunt car</li>
                          <li>1 x SparkFun RedBoard Artemis Nano</li>
                          <li>1 x USB cable</li>
                          <li>2 x Li-Po 3.7V 650mAh (or more) battery</li>
                          <li>2 x Dual motor drivers</li>
                          <li>2 x 4m ToF sensor</li>
                          <li>1 x 9DOF IMU sensor</li>
                          <li>2 x Qwiic connector</li>
                        </ul>
                        <b>Resources:</b>
                        <ul>
                            <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                            <li>https://cwf54.github.io/#interests</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <p>For this lab we did not have a pre-lab.</p>
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <b>Implementing the Stunt Code.</b>
                        <p>The first step was to create code that would allow the stunt to happen. For me, this involved reusing my extrapolation code from Lab 7 to estimate the robot’s distance away from the wall so that the robot knew when to do the flip.</p>
                        <p>As I already mentioned in my Lab 7 report, my extrapolation code consisted of two parts: one for when the TOF sensor data is ready and one for when it isn’t. In the first section, I continuously update two variables as I obtain new data: distance_prev, the distance measurement from the previous iteration, and distance_curr, the new measured distance. In the second section, I use distance_prev and distance_curr to compute the estimated distance, distance_est. I also update distance_prev and distance_curr to keep extrapolating until new data becomes available.</p>
                        <p>For this lab, I just took my extrapolation code from Lab 7 and added some if-statements that determine when the robot should flip. I essentially added the same code to both parts of my Lab 7 code because the general idea is the same: if the robot is more than 500 mm away from the wall, it keeps going forward; if the robot is less than or at 500 mm away from the wall, it suddenly switches direction to induce a flip and keeps going backward for three seconds. The only difference is that in the first section, we look at distance_curr, and in the second section, we look at distance_est. Below is my code for both sections:</p>
                        <p>Code for the first section:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/pt_1.png" alt="pt_1"
                            style="width:425px;height:695px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>Code for the second section:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/pt_2.png" alt="pt_2"
                            style="width:375px;height:750px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <b>Running Into Issues and Debugging.</b>
                        <p>This lab was particularly challenging because I kept running into issues. To deal with these issues, I inserted a lot of print statements into the code to make sure that the coding part was working correctly. I kept debugging until the software worked as it should. These print statements looked something like this in the serial monitor:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/serial_one.png" alt="serial_one"
                            style="width:550px;height:750px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>I also just ran the code with my robot connected to the computer so that I could look at the serial monitor. Once issue that I kept running into was “data underflow”; this would come and go randomly without me changing anything about my code. This hasn’t happened in previous labs. The serial monitor looked like this:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/underflow.png" alt="underflow"
                            style="width:550px;height:650px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>The last and most important issue that I ran into was that the robot was only working “properly” when it was connected to the computer. I tried to debug this by testing out other factors; for instance, I checked that the TOF sensor doesn't read the floor and I tried to charge the Artemis to name a few. However, I never found the true code of this issue. My best guess is faulty and unpredictable hardware.</p>
                        <b>Trials.</b>
                        <p>To attempt to make the robot perform the desired stunt, I just kept trying to repeatedly perform more trials. Below are several of the trials which I managed to record:</p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/9XV8RZM96Ak?feature=share">Trial 1.</a></p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/tsEwQtIat44?feature=share">Trial 2.</a></p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/QVymYeXW1_U?feature=share">Trial 3.</a></p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/aJM1eVGFs0k?feature=share">Trial 4.</a></p>
                        <p> </p>
                        <p><a href="https://youtube.com/shorts/x11qLcgo5oM?feature=share">Trial 5.</a></p>
                        <p> </p>
                        <p>For the last trial, I also made a Distance vs Time plot:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/lab_8_dis.png" alt="lab_8_dis"
                            style="width:475px;height:638px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>As we can see, the TOF data was not being read for some reason, which is why we only see the extrapolated distance being plotted. This relates to the last issue that was mentioned in the previous section. To prove that my code still works as it should, I recorded another trial where I kept the robot connected to the computer and just brought my box towards it. Below is a video recording of the trial and the resulting Distance vs Time plot:</p>
                        <p> </p>
                        <p><a href="https://youtu.be/jc_60yq_g_8">Trial 6.</a></p>
                        <p> </p>
                        <p>Resulting Distance vs Time plot:</p>
                        <p> </p>
                        <img src="assets/img/lab_8/lab_8_conn.png" alt="lab_8_conn"
                            style="width:475px;height:670px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>In Lab 8, I was able to display the current capabilities of my robot by having it perform a stunt. However, in the end, my robot was unable to quickly speed towards a wall, do a flip, and rush towards its original starting point. This is expected because the hardware is very unpredictable and it would have taken many more hours to see the robot flip. My theory is that the robot didn’t flip because it just didn’t react to the wall fast enough. Alternatively, something went wrong when I disconnected the robot from the computer. Because I was short on time, I was forced to move on from this lab. Even though my robot didn’t flip, I still consider this lab a success because even though the hardware didn’t work, my software behaved mostly as I expected it to and I learned a lot of new things.
                        </p>
                      </div>
                   </div>
                </div>
          </section>
          <hr class="m-0" />
        <!-- Lab 9-->
        <section class="resume-section" id="lab9">
          <div class="resume-section-content">
            <h2 class="mb-5">Lab 9</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Mapping (Real).</h3>
                    <div class="subheading mb-3">Introduction.</div>
                    <p>The purpose of Lab 9 was to map a stationary room using our robot. To do that, we had to place our robot in a few marked locations and program it to spin around its axis whale collecting TOF data. The challenge of this task was to make sure that our measurements were consistently separated in angular space so that in the end it was possible to obtain a better map.</p>
                    <b>Materials:</b>
                    <ul>
                      <li>1 x Fully assembled robot, with Artemis, TOF sensors, and an IMU.</li>
                    </ul>
                    <b>Resources:</b>
                    <ul>
                        <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                        <li>https://cwf54.github.io/#interests</li>
                        <li>https://pages.github.coecis.cornell.edu/rc627/ECE-4960/lab9.html</li>
                    </ul>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <div class="subheading mb-3">Procedure.</div>
                    <b>THE PRELAB.</b>
                    <p>For the prelab, we were suggested to go through the lecture of Transformation Matrices.</p>
                    <div class="flex-grow-1"></div>
                    <b>THE LAB TASKS.</b>
                    <b>Angular Speed Control.</b>
                    <p>To complete this lab, I decided to do angular speed control. I implemented it, I split my code into two parts. The first part of the code used the most recent gyroscope data to continuously adjust the PWM value of the wheels. Below is a screenshot of the orientation control code:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/ang_code.png" alt="ang_code"
                        style="width:800px;height:780px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>As we can see, I first find angular velocity by taking gyroscope data. I then use this to compute the yaw value so that I could keep track of the angle that I am at. I also use this value to compute a new PWM value to input into the robot. To actually control how the robot moves, I also defined a function called car_spin that would take as input the PWM and yaw values and use them to control the robot movement. A screenshot of this function is shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/car_spin.png" alt="car_spin"
                        style="width:475px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>As we can see, the code uses the yaw value to check whether the robot has turned 360 degrees or not. If it doesn't, it keeps spinning at the power defined by the given PWM value. If it does, the robot does a hard stop.</p>
                    <p>The second part of my code just collected TOF sensor data. A picture of this part of the code is shown below:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/dis_code.png" alt="dis_code"
                        style="width:1025px;height:500px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <b>Collecting Data.</b>
                    <p>Once I finished and debugged the code, it was time to collect some data. To do that, I had to have my robot spin 360 degrees in four marked locations in the lab. I decided to take two measurements for each location so that in total I had eight measurements. Below is a video demonstrating one of these measurements:</p>
                    <p> </p>
                    <p><a href="https://youtu.be/v8K1Sq8Aat4">Robot Spinning.</a></p>
                    <p> </p>
                    <p>I also included the corresponding Data vs Time plot, Velocity vs Time plot, Yaw vs Time plot, Angular Velocity vs Time plot, PWM vs Time plot, and a polar coordinate plot, all of which are shown below, in that order, with the corresponding code:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/dis_plt.png" alt="dis_plt"
                        style="width:475px;height:600px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/vel_plt.png" alt="vel_plt"
                        style="width:500px;height:600px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/yaw_plt.png" alt="yaw_plt"
                        style="width:460px;height:550px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/ang_vel_plt.png" alt="ang_vel_plt"
                        style="width:455px;height:550px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/pwm_plt.png" alt="pwm_plt.png"
                        style="width:460px;height:450px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/polar_plt.png" alt="polar_plt"
                        style="width:500px;height:690px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>These results are expected because the robot scans distances that were farther and closer to it. Also, I saved the data using pickle so that I could use it to make the map later.</p>
                    <b>Mapping.</b>
                    <p>After having collected all the data, it was time to use it to try to map out the room. First, I had to open all the data that I saved using pickle, process it and save it into arrays:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/pickle.png" alt="pickle"
                        style="width:530px;height:430px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p> </p>
                    <img src="assets/img/lab_9/process.png" alt="process"
                        style="width:700px;height:570px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Once I did that, I had to use transformation matrices to convert the angle and distance measurements to points corresponding to locations in the room. Below is a screenshot of these matrices:</p>
                    <p> </p>
                    <img src="assets/img/lab_9/matrices.png" alt="matrices"
                        style="width:950px;height:500px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>To achieve this, I used Ryan Chan’s plot function. My plotted data as well as the plot function are shown in the screenshot below:</p>
                    p> </p>
                    <img src="assets/img/lab_9/final_plt.png" alt="final_plt"
                        style="width:625px;height:690px;">
                    <div class="flex-grow-1"></div>
                    <p> </p>
                    <p>Unfortunately, it seems like my plot was very busy and I plotted way more points than I needed to. I’m not sure exactly why this happened and whether this was an issue with the data or an issue with my code. However, it does seem like all together my plot shows a very rough, general shape of the room.</p>
                    <p> </p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <div class="subheading mb-3">Conclusion.</div>
                    <p>In Lab 9, I mapped a stationary room by spinning the robot around its axis in several marked locations using angular P control while making TOF sensor measurements. This was a very challenging lab because good measurements were crucial to being able to get a good map. I spent hours and hours redoing my data readings to ensure that the robot spun very close to 360 degrees around its axis at a relatively consistent angular velocity. In the end, we were able to obtain a decent map that … TO BE FINISHED
                    </p>
                  </div>
               </div>
            </div>
      </section>
      <hr class="m-0" />
        <!-- Lab 10 -->
        <section class="resume-section" id="lab10">
           <div class="resume-section-content">
              <h2 class="mb-5">Lab 10</h2>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <h3 class="mb-0">Localization (Simulation).</h3>
                        <div class="subheading mb-3">Introduction.</div>
                        <p>The purpose of Lab 10 is to implement grid localization using the Bayes filter. We did this by running a simulation on Jupyter Notebook because using a simulation would make debugging easier in future labs when we have to implement the Bayes filter on our actual robot. This is because a simulation is a more stable environment than the physical world, meaning that it’s easier to tell if a potential problem is caused by software or hardware.</p>
                        <p> </p>
                        <b>Resources:</b>
                        <ul>
                            <li>https://anyafp.github.io/ece4960/labs/lab10/</li>
                            <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                            <li>https://cwf54.github.io/#interests</li>
                        </ul>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Procedure.</div>
                        <b>THE PRELAB.</b>
                        <p>The primary objective of the prelab is to prepare for the lab by reading background information on the GitHub page and reviewing the relevant lecture content. The main takeaway is that the Bayes filter algorithm uses probability to improve robot localization, the process whereby a robot determines its location in an environment. It does this with a prediction step where the uncertainty of the robot state is increased by incorporating action and an update step where that uncertainty is decreased by incorporating measurement.</p>
                        <div class="flex-grow-1"></div>
                        <b>THE LAB TASKS.</b>
                        <p>For the lab tasks, we had to implement several functions that would allow us to use the Bayes filter algorithm. In total, I had to implement five functions.</p>
                        <b>Implementing Necessary Functions.</b>
                        <p>First, I wrote the compute control function, which implements the equations below:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/equations.png" alt="equations"
                            style="width:520px;height:270px;">
                        <div class="flex-grow-1"></div>
                        <p>These equations describe control variables that are computed based on previous and current position values. Below is a screenshot of my code:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/compute_control.png" alt="compute_control"
                            style="width:989px;height:351px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>Second, was the odometry control function, whose purpose is to compute the probability of the expected versus actual location of the robot. Below is a screenshot of my code:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/omm.png" alt="omm"
                            style="width:914px;height:561px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>Then, I implemented the prediction step function, which updates the current position based on current and previous data readings. Below is a screenshot of my code:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/prediction_step.png" alt="prediction_step"
                            style="width:952px;height:594px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>Afterwards, I wrote the sensor model function, which computes the probability that the robot’s sensor measurements are correct. Below is a screenshot of my code:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/sensor_model.png" alt="sensor_model"
                            style="width:1009px;height:325px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>Lastly, I implemented the update step function, which uses the previous information and measurements to determine the believed current position of the robot. Below is a screenshot of my code:</p>
                        <p> </p>
                        <img src="assets/img/lab_10/update_step.png" alt="update_step"
                            style="width:658px;height:334px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>To implement the functions above I used code from Anya Prabowo’s website.</p>
                        <p> </p>
                        <b>Running the Bayes Filter Algorithm.</b>
                        <p>Once we implemented all the necessary functions, it was time to run the simulation. Below is a video of me running the Bayes filter algorithm followed by an image of the resulting map:</p>
                        <p> </p>
                        <p><a href="https://youtu.be/1dPGpQ8D0b4">Running the Bayes Filter Algorithm.</a></p>
                        <p> </p>
                        <img src="assets/img/lab_10/end_screen.png" alt="end_screen"
                            style="width:864px;height:540px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <img src="assets/img/lab_10/end_map.png" alt="end_map"
                            style="width:600px;height:600px;">
                        <div class="flex-grow-1"></div>
                        <p> </p>
                        <p>In the diagram above, the ground truth is green, odometry is red, and the Bayes filter values are blue.</p>
                    </div>
                </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                        <div class="subheading mb-3">Conclusion.</div>
                        <p>In Lab 10, we used the Bayes filter to implement grid localization in a simulation environment. The Bayes filter allows the robot to estimate its orientation in its environment using probability. The purpose of doing this was to facilitate troubleshooting in Lab 11, where we would have to implement the Bayes filter on the physical robot. Using a simulation makes it easier to narrow the cause of a potential error, like whether it’s a software issue or a hardware issue. Thus, learning how to run the simulation environment could prove to be very useful going forward.
                        </p>
                      </div>
                   </div>
                </div>
          </section>
          <hr class="m-0" />
          <!-- Lab 11-->
          <section class="resume-section" id="lab11">
            <div class="resume-section-content">
              <h2 class="mb-5">Lab 11</h2>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <h3 class="mb-0">Localization (Real).</h3>
                      <div class="subheading mb-3">Introduction.</div>
                      <p>The purpose of Lab 11 was to perform localization on a real robot using the Bayes filter. This involved only using the update step based on 360 degree scans as the prediction step would not have been helpful for our noisy robots. This serves as a follow-up to lab 9, where we performed localization in a simulation.</p>
                      <b>Materials:</b>
                      <ul>
                        <li>1 x Fully assembled robot, with Artemis, TOF sensors, and an IMU.r</li>
                      </ul>
                      <b>Resources:</b>
                      <ul>
                          <li>https://klarinetkat.github.io/ECE4160/#ece4160intro</li>
                          <li>https://cwf54.github.io/#interests</li>
                      </ul>
                  </div>
              </div>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <div class="subheading mb-3">Procedure.</div>
                      <b>THE PRELAB.</b>
                      <p>For the prelab, we were suggested to go through the Sensor Models, Motion Models, and the Bayes Filter lectures.</p>
                      <div class="flex-grow-1"></div>
                      <b>THE LAB TASKS.</b>
                      <b>Setting Up Code Base.</b>
                      <p>The first step of completing this lab was to set up the base code. This involved copying  lab11_sim.ipynb and “lab11_real.ipynb” into the notebooks directory, copying “localization_extras.py” into the root directory, and copying “base_ble.py”, “ble.py”, “connection.yaml” and “cmd_types.py” into the notebooks directory.</p>
                      <b>Localization Simulation.</b>
                      <p>Next, we were asked to test the localization simulation by running “lab11_sim.ipynb”. Below is a screenshot of the final plot that I got:</p>
                      <p> </p>
                      <img src="assets/img/lab_11/sim.png" alt="sim"
                          style="width:725px;height:550px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>In the plot above, the green line represents ground truth, the blue line represents the Bayes filter estimation and the red line represents the odometry reading. As we can see above, the (blue) Bayes filter estimation is very close to the (green) ground truth.</p>
                      <b>Localization Using the Real Robot.</b>
                      <p>The next step was to perform this localization task using our real robot in the “lab11_real.ipynb” notebook.. Instead of doing new measurements, I decided to complete this lab using measurements from lab 9 because I was short on time. This meant that I didn’t have to edit any Arduino code; All I did was open the files in Jupyter using pickle, as shown in the screenshot below:</p>
                      <img src="assets/img/lab_11/pickle.png" alt="pickle"
                          style="width:530px;height:430px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <img src="assets/img/lab_11/process.png" alt="process"
                          style="width:700px;height:570px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>Next, I had to implement a function called perform_observation_loop in the class RealRobot(). This is the only function I really needed to implement because the one above doesn’t really get called. I used Wanda Field’s code as reference. A screenshot of the code is shown below:</p>
                      <p> </p>
                      <img src="assets/img/lab_11/functions.png" alt="process"
                          style="width:775px;height:575px;">
                      <div class="flex-grow-1"></div>
                      <p> </p>
                      <p>Unfortunately I wasn’t able to execute this code fully due to time constraints. Also, there were some issues with my lab 9 code which hindered me from going further. However, I’m sure that if I had more time to edit this it would work properly.</p>
                      <p> </p>
                  </div>
              </div>
              <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                  <div class="flex-grow-1">
                      <div class="subheading mb-3">Conclusion.</div>
                      <p>In Lab 11, we explored how different real-world systems and simulations can be. We did this by comparing applying the Bayes filter in a simulated environment versus doing so on our real robot. Simulations are very optimistic and idealized models of how robots should behave. In real life however, there are a lot of factors which cause our results to stray from simulated ones. Unfortunately I wasn’t able to fully see this distinction with my own robot, but I understand what my results should have looked like and I understood the main lesson and takeaway of this lab.
                      </p>
                    </div>
                 </div>
              </div>
        </section>
        <hr class="m-0" />
          <!-- Lab 12 -->
          <section class="resume-section" id="lab12">
             <div class="resume-section-content">
                <h2 class="mb-5">Lab 12</h2>
                  <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                          <h3 class="mb-0">Planning and Execution.</h3>
                          <div class="subheading mb-3">Introduction.</div>
                          <p>The purpose of Lab 12 is to demonstrate that our robot is able to quickly and accurately navigate through a set of waypoints in a certain environment. In our case, this environment was a small obstacle course in the first room of the lab. The goal was to make our robot follow the following path in a way such that the robot would cross all the points.</p>
                          <b>Materials:</b>
                          <ul>
                            <li>1 x R/C stunt car</li>
                          </ul>
                          <b>Resources:</b>
                          <ul>
                              <li>https://zt88.github.io/Fast-Robots/index.html</li>
                          </ul>
                      </div>
                  </div>
                  <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                          <div class="subheading mb-3">Procedure.</div>
                          <b>THE PRELAB.</b>
                          <p>For this lab we did not have a pre-lab.</p>
                          <div class="flex-grow-1"></div>
                          <b>THE LAB TASKS.</b>
                          <b>Attempt 1: P Control.</b>
                          <p>This lab is unique in comparison to the previous labs because it was quite open-ended; in order to complete the task at hand, we were allowed to use a variety of different methods, including localization, PID, and even open-loop control.</p>
                          <p>For my first attempt, I decided to implement angular and distance P control. To do that, I recycled some of my Lab 9 code as it used angular P control. The original lab 9 code was split into two sections: one that collected gyroscope data and one that collected TOF sensor data. I used the gyroscope data to estimate yaw and I used the distance measurements to do the mapping portion of the assignment. To cater the code to Lab 12, I slightly modified the angular P control section to include all necessary turns and also implemented distance P control.</p>
                          <p>In the end, my code consisted of two parts. The first part ran when IMU data was ready and the second part ran when the TOF data was ready. In the first part, I compute a p value based on the most recent gyroscope reading and call a function named car_spin that makes the robot spin for a certain predetermined angle. Below are the screenshots of my first part code and the car_spin function:</p>
                          <p> </p>
                          <img src="assets/img/lab_12/imu_part.png" alt="imu_part"
                              style="width:755px;height:755px;">
                          <div class="flex-grow-1"></div>
                          <p> </p>
                          <img src="assets/img/lab_12/car_spin.png" alt="car_spin"
                              style="width:605px;height:615px;">
                          <div class="flex-grow-1"></div>
                          <p> </p>
                          <p>In the second part of the code I compute a p value based on the most recent TOF sensor reading and call a function named car_move that uses P control to make my car move a certain distance. Below are the screenshots of my second part code and the car_move function:</p>
                          <p> </p>
                          <img src="assets/img/lab_12/dist_part.png" alt="dist_part"
                              style="width:850px;height:755px;">
                          <div class="flex-grow-1"></div>
                          <p> </p>
                          <img src="assets/img/lab_12/car_move.png" alt="car_move"
                              style="width:600px;height:450px;">
                          <div class="flex-grow-1"></div>
                          <p> </p>
                          <p>To switch between angular control and speed control, I added two global boolean variables, comp_dis and comp_ang. The variable comp_dis tells us whether the robot finished moving a certain distance and comp_ang_tells us whether the robot finished its spin. Depending on which variable is false, the robot executes that motion and sets the global variable back to true.</p>
                          <p>This is quite a promising code that could be very effective after many hours of editing. However, I was running out of time to finish the code for this part, so I decided to try a different approach.</p>
                          <b>Attempt 2: P Control (In Collaboration With Sarika Kannan (sk2446)).</b>
                          <p>Because I and some other students in the lab were doing this lab on the last available day of lab hours, I decided to collaborate with another student to implement PID control in another way. In this way, we were able to get further along and actually see the robot work with the PID. Below are the PID control function that we used and eight MOVE cases each of which was responsible for one of the eight sections of the total trajectory:</p>
                          <script src="https://gist.github.com/sarika2446/0e41571ecbc67a3d2baa5be24b9084ae.js"></script>
                          <script src="https://gist.github.com/sarika2446/0dec5c37ca5bb31aaad5997ea414b692.js"></script>
                          <p>Even though we got further with this PID control code, the robot wasn’t executing the task very well and this would also take many hours to edit.</p>
                          <p>Below are several videos demonstrating the robot trying to navigate the obstacle course:</p>
                          <p> </p>
                          <p><a href="https://youtube.com/shorts/StqK-PwviGk?feature=share">Trial 1.</a></p>
                          <p> </p>
                          <p><a href="https://youtube.com/shorts/CmIOR79k0Ek?feature=share">Trial 2.</a></p>
                          <p> </p>
                          <p><a href="https://youtu.be/WFJ4NYpD9-8">Trial 3.</a></p>
                          <p> </p>
                          <b>Attempt 3: Open Loop Control (In Collaboration With Sarika Kannan (sk2446)).</b>
                          <p>Because a good path planning algorithm using PID control would have taken many more hours to perfect and develop, and we were running out of time, we decided to instead try to implement open loop control. Even though this isn’t a very reliable or robust way to execute this task, it was more time efficient and it was easy to test and debug. We structured the code so that the robot alternated between moving forward and spinning for a set number of milliseconds with a custom PWM value for every movement. Below is our code for the open loop control:</p>
                          <p> </p>
                          <img src="assets/img/lab_12/olc_0.png" alt="olc_0"
                              style="width:630px;height:800px;">
                          <p> </p>
                          <img src="assets/img/lab_12/olc_1.png" alt="olc_1"
                              style="width:630px;height:735px;">
                              <p> </p>
                              <img src="assets/img/lab_12/olc_2.png" alt="olc_2"
                                  style="width:630px;height:720px;">
                              <p> </p>
                              <img src="assets/img/lab_12/olc_3.png" alt="olc_3"
                                  style="width:630px;height:900px;">
                          <div class="flex-grow-1"></div>
                          <p>Ideally, if we had several more hours to tweak the timers and the PWM values, we could have definitely achieved a perfect run. This is because we completed a couple of runs where the robot would perfectly follow half of the trajectory. Unfortunately, my computer died in the middle of testing and a goal that could have been achieved in no more than thirty minutes became an hours-long task. Because lab hours were almost over by that point, we decided to finish at that stage.</p>
                          <p>Below a video demonstrating the robot trying to follow the path:</p>
                          <p> </p>
                          <p><a href="https://youtu.be/0K4a7Q-kwj4">Open Loop Contorl Trial.</a></p>
                          <p> </p>
                          <p>The video above demonstrates our best run, the one that we obtained before my laptop died. If we worked on it for 15-30 more minutes we would be able to follow the entire path.</p>
                          <p> </p>
                      </div>
                  </div>
                  <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                          <div class="subheading mb-3">Conclusion.</div>
                          <p>In Lab 12, I have concluded my journey with Fast Robots by performing a task that required me to make use of a lot of topics that we have studied this semester. I used bluetooth to allow communication between my computer and the robot, I used TOF and IMU sensor readings, I used open loop control and I used PID control. Even though I didn’t have enough time to demonstrate my robot perfectly following the path, I got a great opportunity to have the flexibility to design my solution instead of following strict requirements. Overall, this was quite an interesting lab and a great finale to the course.
                          </p>
                      </div>
                  </div>
                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                    </div>
        </section>
        <hr class="m-0" />
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
</body>

</html>
